{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Requirement already satisfied: torch==1.4.0 in /opt/conda/envs/pytorch_py3/lib/python3.6/site-packages (1.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.4.0\n",
    "#504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from collections import  OrderedDict\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from utils import save_model,test_model,PositoinEncoder,TransformerOptimizer\n",
    "from dataset import MyDataSet\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)                                                                                                                                                                                                          \n",
    "logger.setLevel(level = logging.INFO)\n",
    "handler = logging.FileHandler('age_lstm_dfm5.log')\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "logger.addHandler(handler)\n",
    "logger.addHandler(console)\n",
    "\n",
    "\n",
    "merge_data = pd.read_pickle('prepared_data/merge_data.pkl')\n",
    "user_max_min_dic = pickle.load(open(\"prepared_data/user_max_min_dic\",'rb'))\n",
    "user_df = pd.read_csv('prepared_data/user.csv')\n",
    "\n",
    "use_col = [  'creative_id',  'ad_id', 'product_id',\n",
    "       'product_category', 'advertiser_id', 'industry']\n",
    "\n",
    "#加载预处理的embedding layer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emblayer_dic = {}\n",
    "emblayer_dic[\"user_id\"] = pickle.load(open(\"emb_layer/emb_layer_{}.pkl\".format(\"user_id\"),'rb'))\n",
    "#emblayer_dic[\"creative_id_dw\"] = pickle.load(open(\"emb_layers/emb_layer_{}.pkl\".format(\"creative_iddw\"),'rb'))\n",
    "\n",
    "for col in use_col:\n",
    "    emblayer_dic[col] = pickle.load(open(\"emb_layer/emb_layer_{}.pkl\".format(col),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = []\n",
    "test_ids = []\n",
    "for i in range(3000000):\n",
    "    if i%5==0:\n",
    "        test_ids.append(i+1)\n",
    "    else:\n",
    "        train_ids.append(i+1)\n",
    "train_dst = MyDataSet(train_ids,merge_data ,user_df ,user_max_min_dic,emblayer_dic = emblayer_dic,user_columns =use_col )\n",
    "train_loader = DataLoader(train_dst,num_workers=6,batch_size=400)\n",
    "test_dst = MyDataSet(test_ids,merge_data ,user_df ,user_max_min_dic,user_columns =use_col ,emblayer_dic = emblayer_dic,is_train=False)\n",
    "test_loader = DataLoader(test_dst,num_workers=6,batch_size=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self,modeltype,use_col,dmodel = 512,):\n",
    "        super(MyModel, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(dmodel,nhead=8)\n",
    "        encoder_norm = nn.LayerNorm(dmodel)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, 1, encoder_norm)\n",
    "        self.tfmodel = nn.Transformer(d_model=dmodel, nhead=8, num_encoder_layers=4,\n",
    "                 num_decoder_layers=4, dim_feedforward=dmodel*4, dropout=0.2,\n",
    "                 activation=\"relu\")\n",
    "        self.ufc = nn.Linear(256,dmodel)\n",
    "        #self.user_creative_emb_layer = [emblayer_dic['user_id'].cuda(),None]#emblayer_dic['creative_id_dw'].cuda()]\n",
    "        #self.emb_layers = [emblayer_dic[key].cuda() for key in use_col]\n",
    "        self.char_embedding_size = 0\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.char_embedding_size = 16\n",
    "        #self.emb_lay =  nn.Embedding(92,self.char_embedding_size)\n",
    "        self.cfc = nn.Linear(400*6+1,dmodel)\n",
    "        max_ls = [4445720,3812202,44314,18,62965,335]\n",
    "        #self.emb_layers = [nn.Embedding(key+1,128).cuda() for key in max_ls]\n",
    "        self.ufc = nn.Linear(912+(400*6 +1)*2 ,dmodel)\n",
    "        #self.outfc1 = nn.Linear(dmodel+self.char_embedding_size,64)\n",
    "        self.outfc1 = nn.Linear(dmodel,64)\n",
    "        #out_dim = 10 if modeltype=='age' else 2\n",
    "        self.outfc2 = nn.Linear(64,10)\n",
    "        self.outfc3 = nn.Linear(64,2)\n",
    "        #self.PE = PositoinEncoder(dmodel=dmodel)\n",
    "    def get_dim(self,dim):\n",
    "        return int(12*(1+np.log(dim**0.5)))\n",
    "    def forward(self,uid,raw_arr,click,time):\n",
    "        \"\"\"\n",
    "        模型输入 user_id  user_id 对应的use_col  click_times time\n",
    "        \"\"\"\n",
    "        # embedding 部分\n",
    "        uid = uid.squeeze(1)\n",
    "        #cid = raw_arr\n",
    "        cid = torch.cat([raw_arr,click.unsqueeze(2)],dim=2)\n",
    "        #cid :cat 所有的id emb后的结果，一共4*128 + 64*2 +1 (click_times)维\n",
    "        #uid : deepwalk uid embedding\n",
    "        # 往下是模型部分\n",
    "        cid_sum = cid.sum(dim=1)\n",
    "        cid_max = cid.max(dim=1).values\n",
    "        uid = torch.cat([uid,cid_sum,cid_max],dim=1)\n",
    "        mask = cid.sum(dim=2)\n",
    "        for i,t in enumerate(mask):\n",
    "            t[:torch.where(t!=0)[0].max()+1] = 1\n",
    "        mask = (mask-1).bool()\n",
    "#         size = d_time.size()\n",
    "#         d_time = self.emb_lay(d_time.long().view(size[1]*size[0])).view(size[0],size[1],-1)\n",
    "#         cid = torch.cat([cid,d_time],dim=2)\n",
    "        cid = self.cfc(cid)\n",
    "        cid = self.encoder(cid)\n",
    "        cid = cid.transpose(0,1)\n",
    "        uid = uid.unsqueeze(1)\n",
    "        uid = self.ufc(uid).transpose(0,1)      \n",
    "        out = self.tfmodel(cid,uid,src_key_padding_mask=mask)\n",
    "        out = out.squeeze(0)\n",
    "        out = self.outfc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out1 = self.outfc2(out)\n",
    "        out2 = self.outfc3(out)\n",
    "        return out1,out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(continue_from,train_loader,test_loader):\n",
    "    #continue_from = \"best_transformer_age5.pth\"\n",
    "    modeltype = \"age\"\n",
    "    model = MyModel(modeltype,use_col)\n",
    "    model = model.cuda()\n",
    "\n",
    "    #warm up optimizer\n",
    "    optimizer = TransformerOptimizer(\n",
    "            torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-08),\n",
    "            0.3,\n",
    "            512,\n",
    "            8000)\n",
    "\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(continue_from ):\n",
    "            naive_model = torch.load(continue_from , map_location=lambda storage, loc:storage)\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in naive_model[\"state\"].items():\n",
    "                    new_state_dict[k] = v \n",
    "            model.load_state_dict(new_state_dict)\n",
    "            start_epoch = naive_model[\"epoch\"]+1\n",
    "            optimizer.load_state_dict(naive_model[\"optimizer\"])\n",
    "            optimizer.step_num = naive_model['optimizer_step']\n",
    "            logger.info(\"load model finish\")\n",
    "    losses_age = []\n",
    "    losses_gender= []\n",
    "    best_acc_age = 0 \n",
    "    best_acc_gender = 0 \n",
    "    entroy=nn.CrossEntropyLoss()#mseloss = nn.MSELoss()\n",
    "    model = nn.DataParallel(model)\n",
    "    best_age_acc = 0\n",
    "    best_gender_acc = 0\n",
    "    for epo in range(start_epoch,20):\n",
    "        model.train()\n",
    "        #start = time.time()\n",
    "        for i,(data) in enumerate(train_loader):\n",
    "            raw_arr,click,times,label_age,label_gender,user_id,user_id_emb = data\n",
    "            label_age = label_age.cuda()\n",
    "            label_gender = label_gender.cuda()\n",
    "            click = click.cuda()\n",
    "            times= times.cuda()\n",
    "            user_id_emb = user_id_emb.cuda()\n",
    "            raw_arr= raw_arr.cuda()\n",
    "            embs1,embs2  = model(user_id_emb,raw_arr,click,times)\n",
    "            loss_age = entroy(embs1,label_age-1)\n",
    "            loss_gender = entroy(embs2,label_gender-1)\n",
    "            loss = loss_age+loss_gender\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 400)\n",
    "            optimizer.step()\n",
    "            losses_age.append(loss_age.item())\n",
    "            losses_gender.append(loss_gender.item())\n",
    "            if (i+1)%2000==0: \n",
    "                #print((time.time()-start)/200*len(train_loader))\n",
    "                for g in optimizer.optimizer.param_groups:\n",
    "                    logger.info(\"now learn rate : \"+str(g['lr']) )\n",
    "                mean_loss_age = sum(losses_age)/len(losses_age)\n",
    "                mean_loss_gender = sum(losses_gender)/len(losses_gender)\n",
    "                logger.info('cpEpoch:{0} \\t step:{1}/{2} \\t '\n",
    "                                    'mean loss:{3} \\t{4}'.format(  \n",
    "                                                            (epo + 1), (i + 1),len(train_loader),mean_loss_age,mean_loss_gender))\n",
    "        acc_age,acc_gender,loss_age,loss_gender = test_model(model,test_loader,entroy)\n",
    "        logger.info(\"acc:{},{} loss:{}{}\".format(acc_age,acc_gender,loss_age,loss_gender))\n",
    "        if acc_age>best_acc_age:\n",
    "            best_acc_age = acc_age\n",
    "            save_model(model,epo,optimizer,model_name = continue_from)\n",
    "        if acc_gender>best_gender_acc:\n",
    "            best_gender_acc = acc_gender\n",
    "            save_model(model,epo,optimizer,model_name = continue_from + \"gender.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load model finish\n",
      "now learn rate : 6.320623710592894e-05\n",
      "cpEpoch:8 \t step:2000/6000 \t mean loss:1.225194727599621 \t0.1607104297466576\n",
      "now learn rate : 6.181691938085502e-05\n",
      "cpEpoch:8 \t step:4000/6000 \t mean loss:1.2247975927591324 \t0.16023606195300819\n",
      "now learn rate : 6.051536478449089e-05\n",
      "cpEpoch:8 \t step:6000/6000 \t mean loss:1.2240863166848819 \t0.1596579452926914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1500\n",
      "300 1500\n",
      "600 1500\n",
      "900 1500\n",
      "1200 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc:0.5041933333333334,0.9477433333333334 loss:1.20141958292325350.15217154809087516\n",
      "now learn rate : 5.9292706128157114e-05\n",
      "cpEpoch:9 \t step:2000/6000 \t mean loss:1.221916658371687 \t0.1593578636618331\n",
      "now learn rate : 5.814128183869147e-05\n",
      "cpEpoch:9 \t step:4000/6000 \t mean loss:1.2208348897337913 \t0.15899781892299653\n",
      "now learn rate : 5.70544330734548e-05\n",
      "cpEpoch:9 \t step:6000/6000 \t mean loss:1.2202684439917406 \t0.15889705659387013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1500\n",
      "300 1500\n",
      "600 1500\n",
      "900 1500\n",
      "1200 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc:0.504185,0.94739 loss:1.20604643694559740.15859783238420883\n",
      "now learn rate : 5.6026341062549705e-05\n",
      "cpEpoch:10 \t step:2000/6000 \t mean loss:1.218501488489764 \t0.1584865437171289\n",
      "now learn rate : 5.505189557950345e-05\n",
      "cpEpoch:10 \t step:4000/6000 \t mean loss:1.2171849153488874 \t0.1583700890582986\n",
      "now learn rate : 5.412658773652741e-05\n",
      "cpEpoch:10 \t step:6000/6000 \t mean loss:1.2164273548788493 \t0.1582292478096982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1500\n",
      "300 1500\n",
      "600 1500\n",
      "900 1500\n",
      "1200 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc:0.504425,0.9476416666666667 loss:1.19920076529184970.15357545468211173\n",
      "now learn rate : 5.324642196066232e-05\n",
      "cpEpoch:11 \t step:2000/6000 \t mean loss:1.2148396617591382 \t0.15802505766302347\n",
      "now learn rate : 5.240784322265132e-05\n",
      "cpEpoch:11 \t step:4000/6000 \t mean loss:1.2137330309423533 \t0.15775626227259637\n",
      "now learn rate : 5.1607676490298155e-05\n",
      "cpEpoch:11 \t step:6000/6000 \t mean loss:1.2127920337220033 \t0.1575528852387021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1500\n",
      "300 1500\n",
      "600 1500\n",
      "900 1500\n",
      "1200 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc:0.50525,0.9476766666666666 loss:1.2064536329905190.15253529386719067\n",
      "now learn rate : 5.0843076051247535e-05\n",
      "cpEpoch:12 \t step:2000/6000 \t mean loss:1.211318730546878 \t0.15726249388748637\n",
      "now learn rate : 5.011148285857957e-05\n",
      "cpEpoch:12 \t step:4000/6000 \t mean loss:1.2101764706543514 \t0.15703504726210876\n",
      "now learn rate : 4.941058844013093e-05\n",
      "cpEpoch:12 \t step:6000/6000 \t mean loss:1.209301362514496 \t0.15686737368181347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1500\n",
      "300 1500\n",
      "600 1500\n",
      "900 1500\n",
      "1200 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc:0.5056266666666667,0.9479116666666667 loss:1.20709637920061750.15176120065152646\n",
      "now learn rate : 4.873830421031591e-05\n",
      "cpEpoch:13 \t step:2000/6000 \t mean loss:1.2078748256079852 \t0.15662532100011595\n",
      "now learn rate : 4.80927352539916e-05\n",
      "cpEpoch:13 \t step:4000/6000 \t mean loss:1.2067795534379342 \t0.15639013616894099\n",
      "now learn rate : 4.747215783204687e-05\n",
      "cpEpoch:13 \t step:6000/6000 \t mean loss:1.2059038745727804 \t0.15623053984240526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1500\n",
      "300 1500\n",
      "600 1500\n",
      "900 1500\n",
      "1200 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc:0.50603,0.947465 loss:1.20388102110226950.15176270091036956\n",
      "now learn rate : 4.6875e-05\n",
      "cpEpoch:14 \t step:2000/6000 \t mean loss:1.2046359136575147 \t0.15594997382654172\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "modeltype = 'age'\n",
    "batch_size = 400\n",
    "skf = StratifiedKFold(n_splits=5, random_state=10086, shuffle=True) \n",
    "userid_ls =np.array([i+1 for i in range(3000000)])\n",
    "id_ls =np.array([0 for i in range(3000000)])\n",
    "for index, (train_index, test_index) in enumerate(skf.split(userid_ls.reshape(-1,1),id_ls)):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_x, test_x = userid_ls[train_index,], userid_ls[test_index,]\n",
    "    continue_fom = \"age_tf_pre_encoder__5fload_{}.pth\".format(index)\n",
    "    train_dst = MyDataSet(train_x,merge_data ,user_df ,user_max_min_dic,emblayer_dic = emblayer_dic,user_columns =use_col )\n",
    "    train_loader = DataLoader(train_dst,num_workers=8,batch_size=400,shuffle=True)\n",
    "    test_dst = MyDataSet(test_x,merge_data ,user_df ,user_max_min_dic,user_columns =use_col ,emblayer_dic = emblayer_dic,is_train=False)\n",
    "    test_loader = DataLoader(test_dst,num_workers=8,batch_size=400)\n",
    "    \n",
    "    train(continue_fom,train_loader,test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1500\n",
      "300 1500\n",
      "600 1500\n",
      "900 1500\n",
      "1200 1500\n"
     ]
    }
   ],
   "source": [
    "acc_age,acc_gender,loss_age,loss_gender = test_model(model,test_loader,entroy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load model finish\n"
     ]
    }
   ],
   "source": [
    "modeltype = \"age\"\n",
    "model = MyModel(modeltype,use_col)\n",
    "model = model.cuda()\n",
    "\n",
    "#warm up optimizer\n",
    "optimizer = TransformerOptimizer(\n",
    "        torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-08),\n",
    "        0.3,\n",
    "        512,\n",
    "        8000)\n",
    "\n",
    "start_epoch = 0\n",
    "if os.path.exists(continue_from ):\n",
    "        naive_model = torch.load(continue_from , map_location=lambda storage, loc:storage)\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in naive_model[\"state\"].items():\n",
    "                new_state_dict[k] = v \n",
    "        model.load_state_dict(new_state_dict)\n",
    "        start_epoch = naive_model[\"epoch\"]+1\n",
    "        optimizer.load_state_dict(naive_model[\"optimizer\"])\n",
    "        optimizer.step_num = naive_model['optimizer_step']\n",
    "        logger.info(\"load model finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_from = 'age_tf_pre_encoder__5fload_0.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "entroy=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_py3",
   "language": "python",
   "name": "conda_pytorch_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
